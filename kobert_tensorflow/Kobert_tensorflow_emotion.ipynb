{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFBertModel, BertConfig, BertTokenizer, BertModel, DistilBertModel\n",
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.26.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "140/140 [==============================] - 37s 202ms/step - loss: 1.3248 - accuracy: 0.4171 - val_loss: 0.8702 - val_accuracy: 0.6595\n",
      "Epoch 2/30\n",
      "140/140 [==============================] - 31s 223ms/step - loss: 0.7747 - accuracy: 0.7027 - val_loss: 0.7113 - val_accuracy: 0.7623\n",
      "Epoch 3/30\n",
      "140/140 [==============================] - 33s 238ms/step - loss: 0.6315 - accuracy: 0.7727 - val_loss: 0.7047 - val_accuracy: 0.7435\n",
      "Epoch 4/30\n",
      "140/140 [==============================] - 36s 255ms/step - loss: 0.5050 - accuracy: 0.8207 - val_loss: 0.7096 - val_accuracy: 0.7659\n",
      "Epoch 5/30\n",
      "140/140 [==============================] - 27s 189ms/step - loss: 0.3886 - accuracy: 0.8625 - val_loss: 0.7286 - val_accuracy: 0.7685\n",
      "Epoch 6/30\n",
      "140/140 [==============================] - 23s 162ms/step - loss: 0.2785 - accuracy: 0.9097 - val_loss: 0.8202 - val_accuracy: 0.7685\n",
      "Epoch 7/30\n",
      "140/140 [==============================] - 23s 163ms/step - loss: 0.1945 - accuracy: 0.9390 - val_loss: 1.0040 - val_accuracy: 0.7480\n",
      "Epoch 8/30\n",
      "140/140 [==============================] - 23s 164ms/step - loss: 0.1558 - accuracy: 0.9486 - val_loss: 1.0244 - val_accuracy: 0.7551\n",
      "Epoch 9/30\n",
      "140/140 [==============================] - 23s 164ms/step - loss: 0.1262 - accuracy: 0.9595 - val_loss: 1.1022 - val_accuracy: 0.7498\n",
      "Epoch 10/30\n",
      "140/140 [==============================] - 34s 242ms/step - loss: 0.0740 - accuracy: 0.9776 - val_loss: 1.0882 - val_accuracy: 0.7578\n",
      "Epoch 11/30\n",
      "140/140 [==============================] - 35s 250ms/step - loss: 0.0719 - accuracy: 0.9770 - val_loss: 1.0810 - val_accuracy: 0.7703\n",
      "Epoch 12/30\n",
      "140/140 [==============================] - 27s 190ms/step - loss: 0.0475 - accuracy: 0.9866 - val_loss: 1.1459 - val_accuracy: 0.7542\n",
      "Epoch 13/30\n",
      "140/140 [==============================] - 34s 242ms/step - loss: 0.0405 - accuracy: 0.9866 - val_loss: 1.1643 - val_accuracy: 0.7730\n",
      "Epoch 14/30\n",
      "140/140 [==============================] - 35s 253ms/step - loss: 0.0324 - accuracy: 0.9886 - val_loss: 1.2891 - val_accuracy: 0.7659\n",
      "Epoch 15/30\n",
      "140/140 [==============================] - 29s 204ms/step - loss: 0.0576 - accuracy: 0.9785 - val_loss: 1.1688 - val_accuracy: 0.7730\n",
      "Epoch 16/30\n",
      "140/140 [==============================] - 23s 161ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 1.2862 - val_accuracy: 0.7650\n",
      "Epoch 17/30\n",
      "140/140 [==============================] - 23s 163ms/step - loss: 0.0288 - accuracy: 0.9904 - val_loss: 1.3730 - val_accuracy: 0.7596\n",
      "Epoch 18/30\n",
      "140/140 [==============================] - 23s 162ms/step - loss: 0.0474 - accuracy: 0.9846 - val_loss: 1.2496 - val_accuracy: 0.7676\n",
      "Epoch 19/30\n",
      "140/140 [==============================] - 25s 179ms/step - loss: 0.0395 - accuracy: 0.9866 - val_loss: 1.2583 - val_accuracy: 0.7596\n",
      "Epoch 20/30\n",
      "140/140 [==============================] - 23s 161ms/step - loss: 0.0477 - accuracy: 0.9868 - val_loss: 1.2165 - val_accuracy: 0.7614\n",
      "Epoch 21/30\n",
      "140/140 [==============================] - 23s 162ms/step - loss: 0.0375 - accuracy: 0.9888 - val_loss: 1.2505 - val_accuracy: 0.7721\n",
      "Epoch 22/30\n",
      "140/140 [==============================] - 23s 164ms/step - loss: 0.0253 - accuracy: 0.9926 - val_loss: 1.1815 - val_accuracy: 0.7775\n",
      "Epoch 23/30\n",
      "140/140 [==============================] - 23s 164ms/step - loss: 0.0212 - accuracy: 0.9924 - val_loss: 1.3127 - val_accuracy: 0.7846\n",
      "Epoch 24/30\n",
      "140/140 [==============================] - 23s 162ms/step - loss: 0.0300 - accuracy: 0.9895 - val_loss: 1.3372 - val_accuracy: 0.7605\n",
      "Epoch 25/30\n",
      "140/140 [==============================] - 22s 160ms/step - loss: 0.0216 - accuracy: 0.9922 - val_loss: 1.3521 - val_accuracy: 0.7632\n",
      "Epoch 26/30\n",
      "140/140 [==============================] - 22s 160ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 1.5025 - val_accuracy: 0.7668\n",
      "Epoch 27/30\n",
      "140/140 [==============================] - 22s 159ms/step - loss: 0.0318 - accuracy: 0.9890 - val_loss: 1.4201 - val_accuracy: 0.7489\n",
      "Epoch 28/30\n",
      "140/140 [==============================] - 23s 163ms/step - loss: 0.0145 - accuracy: 0.9958 - val_loss: 1.3663 - val_accuracy: 0.7676\n",
      "Epoch 29/30\n",
      "140/140 [==============================] - 23s 163ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 1.4616 - val_accuracy: 0.7694\n",
      "Epoch 30/30\n",
      "140/140 [==============================] - 22s 160ms/step - loss: 0.0334 - accuracy: 0.9897 - val_loss: 1.3426 - val_accuracy: 0.7730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1acf7772bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: 데이터셋 불러오기\n",
    "train_data = pd.read_csv(\"train_emotion.csv\")\n",
    "train_data.drop(['상황키워드'], axis=1, inplace=True)\n",
    "train_data = train_data.rename(columns={'감정_대분류': 'label', '사람문장': 'document'})\n",
    "train_data.loc[(train_data['label']=='불안'), 'label'] = 0\n",
    "train_data.loc[(train_data['label']=='분노'), 'label'] = 1\n",
    "train_data.loc[(train_data['label']=='상처'), 'label'] = 2 \n",
    "train_data.loc[(train_data['label']=='슬픔'), 'label'] = 3 \n",
    "train_data.loc[(train_data['label']=='기쁨'), 'label'] = 4 \n",
    "train_data = train_data[[\"document\", \"label\"]]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# 라벨링\n",
    "label_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n",
    "train_data['label'] = train_data['label'].apply(lambda x: label_dict[str(x)])\n",
    "\n",
    "# tokenizer 준비\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 입력 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    return input_id, attention_mask\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "MAX_LEN = 50\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in train_data['document']:\n",
    "    input_id, attention_mask = preprocess_sentence(sentence, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# 입력 데이터를 numpy array로 변환\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(train_data['label'])\n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "bert_model = TFBertModel.from_pretrained('monologg/kobert', from_pt=True)\n",
    "input_layer = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
    "attention_layer = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "output_layer = bert_model([input_layer, attention_layer])[1]\n",
    "output_layer = tf.keras.layers.Dense(5, activation='softmax')(output_layer)\n",
    "model = tf.keras.models.Model(inputs=[input_layer, attention_layer], outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([input_ids, attention_masks], labels, epochs=30, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  92186880    ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 5)            3845        ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 92,190,725\n",
      "Trainable params: 92,190,725\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('./model_save/kobert_emotion_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 저장\n",
    "import pickle\n",
    "\n",
    "with open('./model_save/tokenizer_emotion_epoch.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = load_model('./model_save/kobert_emotion_30epoch.h5', custom_objects={'TFBertModel': TFBertModel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 불러오기\n",
    "import pickle\n",
    "\n",
    "with open('./model_save/tokenizer_emotion_30epoch.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from konlpy.tag import Okt\n",
    "import re, pickle\n",
    "import numpy as np\n",
    "\n",
    "class model:\n",
    "  def __init__(self):\n",
    "    self.max_len = 50\n",
    "    self.stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    self.first_model = load_model('MH-1.0.1.h5')\n",
    "    self.second_model = load_model('MH-2.0.1.h5')\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "      self.tokenizer = pickle.load(handle)\n",
    "    self.okt = Okt()\n",
    "\n",
    "    \n",
    "\n",
    "  def sentiment_predict(self, new_sentence):\n",
    "\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = self.okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in self.stopwords] # 불용어 제거\n",
    "    encoded = self.tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = self.max_len) # 패딩\n",
    "    score = float(np.argmax(self.first_model.predict(pad_new), axis=-1)) # 예측\n",
    "    return score\n",
    "\n",
    "  def circumstance_predict(self, new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = self.okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in self.stopwords] # 불용어 제거\n",
    "    encoded = self.tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = self.max_len) # 패딩\n",
    "    score = float(np.argmax(self.second_model.predict(pad_new), axis=-1)) # 예측\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ykh98\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 11s 55ms/step\n",
      "Accuracy: 0.9996424101555516\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 데이터셋 불러오기\n",
    "test_data = pd.read_csv(\"test_emotion.csv\")\n",
    "# 컬럼 이름 변경\n",
    "test_data.drop(['상황키워드'], axis=1, inplace=True)\n",
    "test_data = test_data.rename(columns={'감정_대분류': 'label', '사람문장': 'document'})\n",
    "test_data.loc[(test_data['label']=='불안'), 'label'] = 0\n",
    "test_data.loc[(test_data['label']=='분노'), 'label'] = 1\n",
    "test_data.loc[(test_data['label']=='상처'), 'label'] = 2 \n",
    "test_data.loc[(test_data['label']=='슬픔'), 'label'] = 3 \n",
    "test_data.loc[(test_data['label']=='기쁨'), 'label'] = 4 \n",
    "test_data = test_data[[\"document\", \"label\"]]\n",
    "test_data = test_data.reset_index(drop=True) # 인덱스 리셋\n",
    "\n",
    "# 라벨링\n",
    "label_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n",
    "test_data['label'] = test_data['label'].apply(lambda x: label_dict[str(x)])\n",
    "\n",
    "# tokenizer 준비\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 입력 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    return input_id, attention_mask\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "MAX_LEN = 50\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in test_data['document']:\n",
    "    input_id, attention_mask = preprocess_sentence(sentence, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# 입력 데이터를 numpy array로 변환\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(test_data['label'])\n",
    "\n",
    "# 모델 예측\n",
    "predictions = model.predict([input_ids, attention_masks])\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = np.sum(predicted_labels == labels) / len(labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
