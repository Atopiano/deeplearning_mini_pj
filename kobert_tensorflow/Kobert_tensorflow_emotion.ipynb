{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    TFBertModel,\n",
    "    BertConfig,\n",
    "    BertTokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "# Step 1: 데이터셋 불러오기\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "# 컬럼 이름 변경\n",
    "train_data.drop(['상황키워드'], axis=1, inplace=True)\n",
    "train_data = train_data.rename(columns={'감정_대분류': 'label', '사람문장': 'document'})\n",
    "train_data.loc[(train_data['label']=='불안'), 'label'] = 0\n",
    "train_data.loc[(train_data['label']=='분노'), 'label'] = 1\n",
    "train_data.loc[(train_data['label']=='상처'), 'label'] = 2 \n",
    "train_data.loc[(train_data['label']=='슬픔'), 'label'] = 3 \n",
    "train_data.loc[(train_data['label']=='기쁨'), 'label'] = 4 \n",
    "train_data = train_data[[\"document\", \"label\"]]\n",
    "train_data = train_data.reset_index(drop=True) # 인덱스 리셋\n",
    "\n",
    "# 라벨링\n",
    "label_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n",
    "train_data['label'] = train_data['label'].apply(lambda x: label_dict[str(x)])\n",
    "\n",
    "# tokenizer 준비\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 입력 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    return input_id, attention_mask\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "MAX_LEN = 50\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in train_data['document']:\n",
    "    input_id, attention_mask = preprocess_sentence(sentence, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# 입력 데이터를 numpy array로 변환\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(train_data['label'])\n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "bert_model = TFBertModel.from_pretrained('monologg/kobert', from_pt=True)\n",
    "input_layer = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
    "attention_layer = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "output_layer = bert_model([input_layer, attention_layer])[1]\n",
    "output_layer = tf.keras.layers.Dense(5, activation='softmax')(output_layer)\n",
    "model = tf.keras.models.Model(inputs=[input_layer, attention_layer], outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([input_ids, attention_masks], labels, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('kobert_sentiment_classification.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = load_model('./model_save/kobert_sentiment_classification.h5', custom_objects={'TFBertModel': TFBertModel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ykh98\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 12s 57ms/step\n",
      "Accuracy: 0.9930269980332559\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 데이터셋 불러오기\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "# 컬럼 이름 변경\n",
    "test_data.drop(['상황키워드'], axis=1, inplace=True)\n",
    "test_data = test_data.rename(columns={'감정_대분류': 'label', '사람문장': 'document'})\n",
    "test_data.loc[(test_data['label']=='불안'), 'label'] = 0\n",
    "test_data.loc[(test_data['label']=='분노'), 'label'] = 1\n",
    "test_data.loc[(test_data['label']=='상처'), 'label'] = 2 \n",
    "test_data.loc[(test_data['label']=='슬픔'), 'label'] = 3 \n",
    "test_data.loc[(test_data['label']=='기쁨'), 'label'] = 4 \n",
    "test_data = test_data[[\"document\", \"label\"]]\n",
    "test_data = test_data.reset_index(drop=True) # 인덱스 리셋\n",
    "\n",
    "# 라벨링\n",
    "label_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n",
    "test_data['label'] = test_data['label'].apply(lambda x: label_dict[str(x)])\n",
    "\n",
    "# tokenizer 준비\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 입력 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    return input_id, attention_mask\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "MAX_LEN = 50\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in test_data['document']:\n",
    "    input_id, attention_mask = preprocess_sentence(sentence, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# 입력 데이터를 numpy array로 변환\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(test_data['label'])\n",
    "\n",
    "# 모델 예측\n",
    "predictions = model.predict([input_ids, attention_masks])\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = np.sum(predicted_labels == labels) / len(labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
