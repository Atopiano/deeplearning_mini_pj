{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import TFBertModel, BertConfig, BertTokenizer, BertModel, DistilBertModel\n",
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1291/1291 [==============================] - 277s 206ms/step - loss: 0.9443 - accuracy: 0.6643 - val_loss: 1.4277 - val_accuracy: 0.5279\n",
      "Epoch 2/30\n",
      "1291/1291 [==============================] - 217s 168ms/step - loss: 0.6913 - accuracy: 0.7565 - val_loss: 1.4310 - val_accuracy: 0.5606\n",
      "Epoch 3/30\n",
      "1291/1291 [==============================] - 254s 197ms/step - loss: 0.5852 - accuracy: 0.7935 - val_loss: 1.4311 - val_accuracy: 0.5445\n",
      "Epoch 4/30\n",
      "1291/1291 [==============================] - 281s 218ms/step - loss: 0.4844 - accuracy: 0.8305 - val_loss: 1.6464 - val_accuracy: 0.5437\n",
      "Epoch 5/30\n",
      "1291/1291 [==============================] - 206s 160ms/step - loss: 0.3870 - accuracy: 0.8674 - val_loss: 1.9006 - val_accuracy: 0.5096\n",
      "Epoch 6/30\n",
      "1291/1291 [==============================] - 206s 159ms/step - loss: 0.3040 - accuracy: 0.8994 - val_loss: 1.9803 - val_accuracy: 0.5227\n",
      "Epoch 7/30\n",
      "1291/1291 [==============================] - 206s 160ms/step - loss: 0.2352 - accuracy: 0.9221 - val_loss: 2.0193 - val_accuracy: 0.5231\n",
      "Epoch 8/30\n",
      "1291/1291 [==============================] - 205s 159ms/step - loss: 0.1896 - accuracy: 0.9393 - val_loss: 2.2906 - val_accuracy: 0.5033\n",
      "Epoch 9/30\n",
      "1291/1291 [==============================] - 205s 159ms/step - loss: 0.1507 - accuracy: 0.9527 - val_loss: 2.6811 - val_accuracy: 0.4985\n",
      "Epoch 10/30\n",
      "1291/1291 [==============================] - 206s 159ms/step - loss: 0.1282 - accuracy: 0.9597 - val_loss: 2.5717 - val_accuracy: 0.5140\n",
      "Epoch 11/30\n",
      "1291/1291 [==============================] - 206s 159ms/step - loss: 0.1103 - accuracy: 0.9652 - val_loss: 2.6608 - val_accuracy: 0.5176\n",
      "Epoch 12/30\n",
      "1291/1291 [==============================] - 204s 158ms/step - loss: 0.0945 - accuracy: 0.9697 - val_loss: 2.6448 - val_accuracy: 0.5226\n",
      "Epoch 13/30\n",
      "1291/1291 [==============================] - 201s 156ms/step - loss: 0.0783 - accuracy: 0.9755 - val_loss: 2.9006 - val_accuracy: 0.5242\n",
      "Epoch 14/30\n",
      "1291/1291 [==============================] - 201s 156ms/step - loss: 0.0777 - accuracy: 0.9753 - val_loss: 3.0585 - val_accuracy: 0.5293\n",
      "Epoch 15/30\n",
      "1291/1291 [==============================] - 201s 155ms/step - loss: 0.0693 - accuracy: 0.9783 - val_loss: 3.0376 - val_accuracy: 0.5123\n",
      "Epoch 16/30\n",
      "1291/1291 [==============================] - 205s 159ms/step - loss: 0.0604 - accuracy: 0.9812 - val_loss: 3.0200 - val_accuracy: 0.5241\n",
      "Epoch 17/30\n",
      "1291/1291 [==============================] - 205s 159ms/step - loss: 0.0627 - accuracy: 0.9792 - val_loss: 3.1410 - val_accuracy: 0.5227\n",
      "Epoch 18/30\n",
      "1291/1291 [==============================] - 206s 159ms/step - loss: 0.0503 - accuracy: 0.9844 - val_loss: 3.1133 - val_accuracy: 0.5404\n",
      "Epoch 19/30\n",
      "1291/1291 [==============================] - 206s 159ms/step - loss: 0.0504 - accuracy: 0.9839 - val_loss: 3.1391 - val_accuracy: 0.5279\n",
      "Epoch 20/30\n",
      "1291/1291 [==============================] - 203s 157ms/step - loss: 0.0498 - accuracy: 0.9837 - val_loss: 3.1983 - val_accuracy: 0.5240\n",
      "Epoch 21/30\n",
      "1291/1291 [==============================] - 201s 156ms/step - loss: 0.0467 - accuracy: 0.9853 - val_loss: 3.2773 - val_accuracy: 0.5288\n",
      "Epoch 22/30\n",
      "1291/1291 [==============================] - 201s 156ms/step - loss: 0.0461 - accuracy: 0.9861 - val_loss: 3.3901 - val_accuracy: 0.5167\n",
      "Epoch 23/30\n",
      "1291/1291 [==============================] - 200s 155ms/step - loss: 0.0406 - accuracy: 0.9873 - val_loss: 3.1116 - val_accuracy: 0.5323\n",
      "Epoch 24/30\n",
      "1291/1291 [==============================] - 201s 156ms/step - loss: 0.0407 - accuracy: 0.9877 - val_loss: 3.1306 - val_accuracy: 0.5235\n",
      "Epoch 25/30\n",
      "1291/1291 [==============================] - 201s 156ms/step - loss: 0.0406 - accuracy: 0.9872 - val_loss: 3.4158 - val_accuracy: 0.5150\n",
      "Epoch 26/30\n",
      "1291/1291 [==============================] - 201s 155ms/step - loss: 0.0345 - accuracy: 0.9890 - val_loss: 3.4747 - val_accuracy: 0.5247\n",
      "Epoch 27/30\n",
      "1291/1291 [==============================] - 201s 155ms/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 3.2086 - val_accuracy: 0.5250\n",
      "Epoch 28/30\n",
      "1291/1291 [==============================] - 201s 155ms/step - loss: 0.0346 - accuracy: 0.9890 - val_loss: 3.4306 - val_accuracy: 0.5239\n",
      "Epoch 29/30\n",
      "1291/1291 [==============================] - 201s 155ms/step - loss: 0.0346 - accuracy: 0.9890 - val_loss: 3.4649 - val_accuracy: 0.5329\n",
      "Epoch 30/30\n",
      "1291/1291 [==============================] - 200s 155ms/step - loss: 0.0322 - accuracy: 0.9900 - val_loss: 3.4728 - val_accuracy: 0.5205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d47bd835b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "# Step 1: 데이터셋 불러오기\n",
    "train_data = pd.read_csv(\"9keywords_train.csv\")\n",
    "# 컬럼 이름 변경\n",
    "train_data.drop(['감정_대분류'], axis=1, inplace=True)\n",
    "train_data = train_data.rename(columns={'상황키워드': 'label', '사람문장': 'document'})\n",
    "train_data.loc[(train_data['label']=='대인관계'), 'label'] = 0\n",
    "train_data.loc[(train_data['label']=='진로,취업,직장'), 'label'] = 1\n",
    "train_data.loc[(train_data['label']=='연애,결혼,출산'), 'label'] = 2 \n",
    "train_data.loc[(train_data['label']=='가족관계'), 'label'] = 3 \n",
    "train_data.loc[(train_data['label']=='학업 및 진로'), 'label'] = 4 \n",
    "train_data.loc[(train_data['label']=='학교폭력/따돌림'), 'label'] = 5\n",
    "train_data.loc[(train_data['label']=='재정,은퇴,노후준비'), 'label'] = 6\n",
    "train_data.loc[(train_data['label']=='직장, 업무 스트레스'), 'label'] = 7\n",
    "train_data.loc[(train_data['label']=='건강,죽음'), 'label'] = 8\n",
    "\n",
    "train_data = train_data[[\"document\", \"label\"]]\n",
    "train_data = train_data.reset_index(drop=True) # 인덱스 리셋\n",
    "\n",
    "# 라벨링\n",
    "label_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8}\n",
    "train_data['label'] = train_data['label'].apply(lambda x: label_dict[str(x)])\n",
    "\n",
    "# tokenizer 준비\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 입력 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    return input_id, attention_mask\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "MAX_LEN = 50\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in train_data['document']:\n",
    "    input_id, attention_mask = preprocess_sentence(sentence, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# 입력 데이터를 numpy array로 변환\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(train_data['label'])\n",
    "\n",
    "# 모델 생성 및 훈련\n",
    "bert_model = TFBertModel.from_pretrained('monologg/kobert', from_pt=True)\n",
    "input_layer = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
    "attention_layer = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "output_layer = bert_model([input_layer, attention_layer])[1]\n",
    "output_layer = tf.keras.layers.Dense(9, activation='softmax')(output_layer)\n",
    "model = tf.keras.models.Model(inputs=[input_layer, attention_layer], outputs=output_layer)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([input_ids, attention_masks], labels, epochs=30, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('./model_save/kobert_situation_epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 저장\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer_situation_epochs.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer 불러오기\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer_situation_epochs.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = load_model('./model_save/kobert_situation_epochs.h5', custom_objects={'TFBertModel': TFBertModel})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from konlpy.tag import Okt\n",
    "import re, pickle\n",
    "import numpy as np\n",
    "\n",
    "class model:\n",
    "  def __init__(self):\n",
    "    self.max_len = 50\n",
    "    self.stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "    self.first_model = load_model('MH-1.0.1.h5')\n",
    "    self.second_model = load_model('MH-2.0.1.h5')\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "      self.tokenizer = pickle.load(handle)\n",
    "    self.okt = Okt()\n",
    "\n",
    "    \n",
    "\n",
    "  def sentiment_predict(self, new_sentence):\n",
    "\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = self.okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in self.stopwords] # 불용어 제거\n",
    "    encoded = self.tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = self.max_len) # 패딩\n",
    "    score = float(np.argmax(self.first_model.predict(pad_new), axis=-1)) # 예측\n",
    "    return score\n",
    "\n",
    "  def circumstance_predict(self, new_sentence):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "    new_sentence = self.okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in self.stopwords] # 불용어 제거\n",
    "    encoded = self.tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "    pad_new = pad_sequences(encoded, maxlen = self.max_len) # 패딩\n",
    "    score = float(np.argmax(self.second_model.predict(pad_new), axis=-1)) # 예측\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\ykh98\\anaconda3\\envs\\tf2\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 13s 55ms/step\n",
      "Accuracy: 0.8272850474326156\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 데이터셋 불러오기\n",
    "test_data = pd.read_csv(\"9keywords_test.csv\")\n",
    "# 컬럼 이름 변경\n",
    "test_data.drop(['감정_대분류'], axis=1, inplace=True)\n",
    "test_data = test_data.rename(columns={'상황키워드': 'label', '사람문장': 'document'})\n",
    "test_data.loc[(test_data['label']=='대인관계'), 'label'] = 0\n",
    "test_data.loc[(test_data['label']=='진로,취업,직장'), 'label'] = 1\n",
    "test_data.loc[(test_data['label']=='연애,결혼,출산'), 'label'] = 2 \n",
    "test_data.loc[(test_data['label']=='가족관계'), 'label'] = 3 \n",
    "test_data.loc[(test_data['label']=='학업 및 진로'), 'label'] = 4 \n",
    "test_data.loc[(test_data['label']=='학교폭력/따돌림'), 'label'] = 5\n",
    "test_data.loc[(test_data['label']=='재정,은퇴,노후준비'), 'label'] = 6\n",
    "test_data.loc[(test_data['label']=='직장, 업무 스트레스'), 'label'] = 7\n",
    "test_data.loc[(test_data['label']=='건강,죽음'), 'label'] = 8\n",
    "test_data = test_data[[\"document\", \"label\"]]\n",
    "test_data = test_data.reset_index(drop=True) # 인덱스 리셋\n",
    "\n",
    "# 라벨링\n",
    "label_dict = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8}\n",
    "test_data['label'] = test_data['label'].apply(lambda x: label_dict[str(x)])\n",
    "\n",
    "# tokenizer 준비\n",
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')\n",
    "\n",
    "# 입력 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text=sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    return input_id, attention_mask\n",
    "\n",
    "# 입력 데이터 전처리\n",
    "MAX_LEN = 50\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for sentence in test_data['document']:\n",
    "    input_id, attention_mask = preprocess_sentence(sentence, MAX_LEN)\n",
    "    input_ids.append(input_id)\n",
    "    attention_masks.append(attention_mask)\n",
    "\n",
    "# 입력 데이터를 numpy array로 변환\n",
    "input_ids = np.array(input_ids)\n",
    "attention_masks = np.array(attention_masks)\n",
    "labels = np.array(test_data['label'])\n",
    "\n",
    "# 모델 예측\n",
    "predictions = model.predict([input_ids, attention_masks])\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 정확도 계산\n",
    "accuracy = np.sum(predicted_labels == labels) / len(labels)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
